In this step, we will take a snapshot of all volumes for our Cassandra cluster, then drop our database table.

### Step: Take snapshot using kubectl

First let's insert a new record in our features table so we can show that the snapshot will take the latest available data:
```
kubectl exec -it cqlsh -- cqlsh cassandra-0.cassandra.default.svc.cluster.local --cqlversion=3.4.4
INSERT INTO portworx.features (id, name, value) VALUES ('px-6', '3DSnaps', 'Application/Cluster aware snapshots!');
SELECT id, name, value FROM portworx.features;
quit
```{{execute T1}}

We're going to use STORK to take a 3DSnapshot of our Cassandra cluster. Take a look at the px-snap.yaml file ```cat px-snap.yaml ```{{execute T1}} and notice that we are going to force a ```nodetool flush``` command on each cluster member before we take the snapshot. As explained before, that will force all data to be written to disk in order to ensure consistency of the snapshot. We also defined the volume group name (cassandra_vg) so Portworx will synchronously quiesce I/O on all volumes before triggering their snapshots.

Now let's take a snapshot.
```
kubectl create -f px-snap.yaml
```{{execute T1}}

You can see the snapshots using the following command:
```
watch kubectl get volumesnapshots
```{{execute T1}}

When you see all volumesnapshots appear, click ```clear```{{execute interrupt}} to ctrl-c and clear the screen.

### Step: Drop features table

Now we're going to go ahead and do something stupid because it's Katacoda and we're here to learn.

```
kubectl exec -it cqlsh -- cqlsh cassandra-0.cassandra.default.svc.cluster.local --cqlversion=3.4.4
DROP TABLE IF EXISTS portworx.features;
SELECT id, name, value FROM portworx.features;
quit
```{{execute T1}}

You should have received an "Error" since the table is deleted. Ok, so we deleted our database, what now? 

Create clones from your snapshots and restore from those snapshots.

First edit vols-from-snaps and inset the volumesnapshots from the above "kubectl get volumesnapshots" output.

 `vi vols-from-snaps.yaml`{{execute T1}}

 Then create the clones.
```
kubectl create -f vols-from-snaps.yaml
```{{execute T1}}

View the PVCs
```
kubectl get pvcs
```{{execute T1}}

Restore cassandra. We delete the original Cassandra deployment only because we dont have enough nodes in this lab to host two. Then we create the new cassandra statefulset based on our cloned snapshots.
```
kubectl delete -f cassandra.yaml
kubectl create -f cassandra-app-restore.yaml
```{{execute T1}}

Wait for restored cassandra database to be Running (1/1)
```
watch kubectl get po
```{{execute T1}}

When you see all pods running, click ```clear```{{execute interrupt}} to ctrl-c and clear the screen.

New let's verify the data is restored.

Start a CQL Shell session:
```
kubectl exec -it cqlsh-restored -- cqlsh cassandra-restored-0.cassandra-restored.default.svc.cluster.local --cqlversion=3.4.4
```{{execute T1}}

Select rows from the keyspace we previously created:
```
SELECT id, name, value FROM portworx.features;
```{{execute T1}}

You have now restored from a snapshot! Go ahead and ```quit```{{execute T1}} the cqlsh session before finishing.




